{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebeccaalmendra/4.1-Project/blob/main/milestone2\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd-3RenSrgBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb10f548-5228-48f4-b8e5-58685b644ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n",
            "Number of comments: 1095\n",
            "0 comments processed.\n",
            "200 comments processed.\n",
            "400 comments processed.\n",
            "600 comments processed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800 comments processed.\n",
            "1000 comments processed.\n",
            "Finished preprocessing.\n",
            "\n",
            "Topic 1: market, money, stock, inflation, people, year, like, even, company, time\n",
            "\n",
            "Topic 2: inflation, keynesian, government, economy, every, hard, result, argentina, higher, economics\n",
            "\n",
            "Topic 3: company, billion, elon, dollar, people, energy, green, money, industry, get\n",
            "\n",
            "Topic 4: inflation, year, price, get, cumulative, stock, really, number, say, hearing\n",
            "\n",
            "Topic 5: time, company, dollar, ratio, economy, stock, buy, growth, luck, bubble\n",
            "\n",
            "Topic 6: stock, would, get, year, energy, government, like, green, way, elon\n",
            "\n",
            "Topic 7: stock, growth, government, took, value, economy, market, higher, job, hard\n",
            "\n",
            "Topic 8: stock, money, ratio, buy, even, price, get, chart, bubble, spy\n",
            "\n",
            "Topic 9: market, ratio, year, energy, growth, chart, one, green, spy, bubble\n",
            "\n",
            "Topic 10: people, think, job, much, year, thats, value, long, term, lot\n",
            "\n",
            "==================================================\n",
            "TOP 3 BUZZ WORDS (Most trending across all topics):\n",
            "==================================================\n",
            "1. 'STOCK' - appears in 6 topics\n",
            "2. 'YEAR' - appears in 5 topics\n",
            "3. 'GET' - appears in 4 topics\n",
            "\n",
            "These are the most discussed terms in r/wallstreetbets right now!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "import praw\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# Suppress the async warning in Google Colab\n",
        "warnings.filterwarnings(\"ignore\", message=\"It appears that you are using PRAW in an asynchronous environment\")\n",
        "\n",
        "# Reddit login\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"RErsAg2Ro6w2jo9Fbwgo4Q\",\n",
        "    client_secret=\"eDHBEx5VWeqvPqwgRVzsGeXqnhQkLw\",\n",
        "    username=\"Bright-Step1881\",\n",
        "    password=\"wxy1vkq9erm*NQR8hcj\",\n",
        "    user_agent=\"BUS_498 web scraper\"\n",
        ")\n",
        "\n",
        "# Link to the thread we're using\n",
        "url = \"https://www.reddit.com/r/wallstreetbets/comments/1hrw8ke/us_stock_market_is_more_expensive_than_at_the/\"\n",
        "\n",
        "# Get the individual posts (even though there isn't a loop, don't worry, the submission method is iterative)\n",
        "submission = reddit.submission(url=url)\n",
        "\n",
        "# This is a praw method, it gets the replies to the comments which the submission doesn't do\n",
        "submission.comments.replace_more(limit=None)\n",
        "\n",
        "# Break up the comments into dicts for different types of info\n",
        "comments = []\n",
        "for comment in submission.comments.list():\n",
        "    comments.append({\n",
        "        \"id\": comment.id,\n",
        "        \"author\": str(comment.author),\n",
        "        \"body\": comment.body,\n",
        "        \"created_utc\": pd.to_datetime(comment.created_utc, unit=\"s\"),  # UTC stands for coordinated universal time and is how reddit timestamps its posts\n",
        "        \"score\": comment.score,\n",
        "        \"parent_id\": comment.parent_id\n",
        "    })\n",
        "\n",
        "# Save\n",
        "df = pd.DataFrame(comments)\n",
        "df.to_csv(\"wsb_comments.csv\", index=False)\n",
        "\n",
        "# Test to see if it worked\n",
        "print(f\"Number of comments: {len(df)}\")\n",
        "\n",
        "# Import additional libraries for text processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "except:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove HTML tags from text\"\"\"\n",
        "    p = re.compile('<.*?>')\n",
        "    return p.sub(' ', text)\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Convert treebank POS tag to wordnet POS tag\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return \"a\"  # adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return \"v\"  # verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return \"n\"  # noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return \"r\"  # adverb\n",
        "    return \"n\"  # default to noun\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    \"\"\"Lemmatize tokens using basic lemmatization\"\"\"\n",
        "    tokens_new = []\n",
        "    wn_lt = WordNetLemmatizer()\n",
        "    for word in tokens:\n",
        "        tokens_new.append(wn_lt.lemmatize(word))\n",
        "    return tokens_new\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Remove stopwords from tokens\"\"\"\n",
        "    nltk_sw = stopwords.words('english')\n",
        "    # Add common fragments and short words to filter out\n",
        "    custom_stopwords = nltk_sw + ['wa', 'im', 've', 'll', 're', 'd', 's', 't']\n",
        "    return [word for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = remove_html_tags(text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|[^a-zA-Z\\s]\", '', text)\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r\"\\b[\\w']+\\b\")\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = lemmatize(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess all comments\n",
        "corpus = []\n",
        "for idx, text in enumerate(df[\"body\"]):\n",
        "    if idx % 200 == 0:\n",
        "        print(f\"{idx} comments processed.\")\n",
        "    corpus.append(preprocess(text))\n",
        "\n",
        "print(\"Finished preprocessing.\")\n",
        "\n",
        "# Bag of Words + LSA\n",
        "bow_corpus = [\" \".join(tokens) for tokens in corpus]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(bow_corpus)\n",
        "\n",
        "# Perform SVD for Latent Semantic Analysis\n",
        "U, S, VT = randomized_svd(X, n_components=10, n_iter=7, random_state=42)\n",
        "\n",
        "# Show Top 10 Words Per Topic\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "all_topic_words = []\n",
        "\n",
        "for i, comp in enumerate(VT):\n",
        "    top_indices = comp.argsort()[-10:][::-1]\n",
        "    top_words = [terms[j] for j in top_indices]\n",
        "    all_topic_words.extend(top_words)  # Collect all topic words\n",
        "    print(f\"\\nTopic {i+1}: {', '.join(top_words)}\")\n",
        "\n",
        "# Find buzz words - most frequent words across all topics\n",
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(all_topic_words)\n",
        "top_buzz_words = word_counts.most_common(3)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"TOP 3 BUZZ WORDS (Most trending across all topics):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, (word, count) in enumerate(top_buzz_words, 1):\n",
        "    print(f\"{i}. '{word.upper()}' - appears in {count} topics\")\n",
        "\n",
        "print(f\"\\nThese are the most discussed terms in r/wallstreetbets right now!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmqrX5WCehu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis: The second mining method we will be using is Latent Semantic Analysis (LSA) which uses Singular Value Decomposition (SVD) to relate recurring words within our collection of reddit comments. This is suited well for the data set we have chosen because there are often many recurring terms used in finances such as specific stocks, words like up, down, “to the moon”, etc.\n",
        "\n",
        "The inputs will be the comment text converted into a TF-IDF weighted term matrix, where rows will represent the individual comments, and columns represent unique words.\n",
        "\n",
        "The output will be scores for comments based on:\n",
        " topic_assignment (which main topic each comment belongs to, e.g., Topic 1, Topic 2, etc.)\n",
        "topic_keywords (the most important words that define each discovered topic)\n",
        "topic_scores (how strongly each comment relates to each topic on a 0-1 scale)\n"
      ],
      "metadata": {
        "id": "Mf92B4brtQFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Code"
      ],
      "metadata": {
        "id": "zlbJZ0eaAKXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# Suppress all PRAW warnings in Google Colab\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"praw\").setLevel(logging.ERROR)\n",
        "\n",
        "# Reddit login\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"RErsAg2Ro6w2jo9Fbwgo4Q\",\n",
        "    client_secret=\"eDHBEx5VWeqvPqwgRVzsGeXqnhQkLw\",\n",
        "    username=\"Bright-Step1881\",\n",
        "    password=\"wxy1vkq9erm*NQR8hcj\",\n",
        "    user_agent=\"BUS_498 web scraper\"\n",
        ")\n",
        "\n",
        "# Link to the thread we're using\n",
        "url = \"https://www.reddit.com/r/wallstreetbets/comments/1hrw8ke/us_stock_market_is_more_expensive_than_at_the/\"\n",
        "\n",
        "# Get the individual posts (even though there isn't a loop, don't worry, the submission method is iterative)\n",
        "submission = reddit.submission(url=url)\n",
        "\n",
        "# This is a praw method, it gets the replies to the comments which the submission doesn't do\n",
        "submission.comments.replace_more(limit=None)\n",
        "\n",
        "# Break up the comments into dicts for different types of info\n",
        "comments = []\n",
        "for comment in submission.comments.list():\n",
        "    comments.append({\n",
        "        \"id\": comment.id,\n",
        "        \"author\": str(comment.author),\n",
        "        \"body\": comment.body,\n",
        "        \"created_utc\": pd.to_datetime(comment.created_utc, unit=\"s\"),  # UTC stands for coordinated universal time and is how reddit timestamps its posts\n",
        "        \"score\": comment.score,\n",
        "        \"parent_id\": comment.parent_id\n",
        "    })\n",
        "\n",
        "# Save\n",
        "df = pd.DataFrame(comments)\n",
        "df.to_csv(\"wsb_comments.csv\", index=False)\n",
        "\n",
        "# Test to see if it worked\n",
        "print(f\"Number of comments: {len(df)}\")\n",
        "\n",
        "# Import additional libraries for text processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "except:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove HTML tags from text\"\"\"\n",
        "    p = re.compile('<.*?>')\n",
        "    return p.sub(' ', text)\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Convert treebank POS tag to wordnet POS tag\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return \"a\"  # adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return \"v\"  # verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return \"n\"  # noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return \"r\"  # adverb\n",
        "    return \"n\"  # default to noun\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    \"\"\"Lemmatize tokens using basic lemmatization\"\"\"\n",
        "    tokens_new = []\n",
        "    wn_lt = WordNetLemmatizer()\n",
        "    for word in tokens:\n",
        "        tokens_new.append(wn_lt.lemmatize(word))\n",
        "    return tokens_new\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Remove stopwords from tokens\"\"\"\n",
        "    nltk_sw = stopwords.words('english')\n",
        "    # Add common fragments and short words to filter out\n",
        "    custom_stopwords = nltk_sw + ['wa', 'im', 've', 'll', 're', 'd', 's', 't']\n",
        "    return [word for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = remove_html_tags(text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|[^a-zA-Z\\s]\", '', text)\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r\"\\b[\\w']+\\b\")\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = lemmatize(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess all comments\n",
        "corpus = []\n",
        "for idx, text in enumerate(df[\"body\"]):\n",
        "    if idx % 200 == 0:\n",
        "        print(f\"{idx} comments processed.\")\n",
        "    corpus.append(preprocess(text))\n",
        "\n",
        "print(\"Finished preprocessing.\")\n",
        "\n",
        "# Bag of Words + LSA\n",
        "bow_corpus = [\" \".join(tokens) for tokens in corpus]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(bow_corpus)\n",
        "\n",
        "# Perform SVD for Latent Semantic Analysis\n",
        "U, S, VT = randomized_svd(X, n_components=10, n_iter=7, random_state=42)\n",
        "\n",
        "# Show Top 10 Words Per Topic\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "all_topic_words = []\n",
        "\n",
        "for i, comp in enumerate(VT):\n",
        "    top_indices = comp.argsort()[-10:][::-1]\n",
        "    top_words = [terms[j] for j in top_indices]\n",
        "    all_topic_words.extend(top_words)  # Collect all topic words\n",
        "    print(f\"\\nTopic {i+1}: {', '.join(top_words)}\")\n",
        "\n",
        "# Find buzz words - most frequent words across all topics\n",
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(all_topic_words)\n",
        "top_buzz_words = word_counts.most_common(3)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"TOP 3 BUZZ WORDS (Most trending across all topics):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, (word, count) in enumerate(top_buzz_words, 1):\n",
        "    print(f\"{i}. '{word.upper()}' - appears in {count} topics\")\n",
        "\n",
        "print(f\"\\nThese are the most discussed terms in r/wallstreetbets right now!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "JxZcWHRbdyLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec10ce18-f56f-4c4a-baa3-e2d6ac00735e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of comments: 1095\n",
            "0 comments processed.\n",
            "200 comments processed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400 comments processed.\n",
            "600 comments processed.\n",
            "800 comments processed.\n",
            "1000 comments processed.\n",
            "Finished preprocessing.\n",
            "\n",
            "Topic 1: market, money, stock, inflation, people, year, like, even, company, time\n",
            "\n",
            "Topic 2: inflation, keynesian, government, economy, every, hard, result, argentina, higher, economics\n",
            "\n",
            "Topic 3: company, billion, elon, dollar, people, energy, green, money, industry, get\n",
            "\n",
            "Topic 4: inflation, year, price, get, cumulative, stock, really, number, say, hearing\n",
            "\n",
            "Topic 5: time, company, dollar, ratio, economy, stock, buy, growth, luck, bubble\n",
            "\n",
            "Topic 6: stock, would, get, year, energy, government, like, green, way, elon\n",
            "\n",
            "Topic 7: stock, growth, government, took, value, economy, market, higher, job, hard\n",
            "\n",
            "Topic 8: stock, money, ratio, buy, even, price, get, chart, bubble, spy\n",
            "\n",
            "Topic 9: market, ratio, year, energy, growth, chart, one, green, spy, bubble\n",
            "\n",
            "Topic 10: people, think, job, much, year, thats, value, long, term, lot\n",
            "\n",
            "==================================================\n",
            "TOP 3 BUZZ WORDS (Most trending across all topics):\n",
            "==================================================\n",
            "1. 'STOCK' - appears in 6 topics\n",
            "2. 'YEAR' - appears in 5 topics\n",
            "3. 'GET' - appears in 4 topics\n",
            "\n",
            "These are the most discussed terms in r/wallstreetbets right now!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.shape) #num of comments and vocab size"
      ],
      "metadata": {
        "id": "3ziX4VH44lr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, S, VT = randomized_svd(X, n_components=10, n_iter=7, random_state=42)"
      ],
      "metadata": {
        "id": "SVVw8mm_5Z-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vqu0Z9crVZLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K98U4NaQALip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vCph0jKW5fqC"
      }
    }
  ]
}